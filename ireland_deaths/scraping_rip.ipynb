{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4327582",
   "metadata": {},
   "source": [
    "# web scraping RIP.ie with beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2fc234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datefinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7726f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_dn_page_old(n):\n",
    "    \"\"\"Get death notice text from page matching the id number\"\"\"\n",
    "    \n",
    "    url = 'https://rip.ie/showdn.php?dn=%s' %n\n",
    "\n",
    "    user_agent = 'Chrome/107.0.5304.110'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    req = requests.get(url,headers=headers)   \n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    if soup.title == None:\n",
    "        title = ''\n",
    "    else:        \n",
    "        title  = soup.title.text.strip()\n",
    "    name=''    \n",
    "    for s in ['Death Notice of ','The death has occurred of ']:\n",
    "        if title.startswith(s):\n",
    "            name = title.split(s)[1]   \n",
    "    elem = soup.find_all(\"div\", id=\"dn_photo_and_text\")\n",
    "    \n",
    "    if len(elem) == 0:\n",
    "        return name, '', '', '', ''\n",
    "    rows = elem[0].find_all('p')\n",
    "    if len(rows) == 0:\n",
    "        rows = elem[0].find_all('td')\n",
    "    text = ';'.join([r.text.strip() for r in rows]).replace('\\n','')\n",
    "    #address\n",
    "    addrelem = soup.find(\"span\", class_='small_addr') \n",
    "    if addrelem != None:\n",
    "        address = addrelem.text.strip()\n",
    "    else:\n",
    "        address = ''\n",
    "    #county  \n",
    "    ctyelem = soup.find(\"li\", class_='fd_county') \n",
    "    if ctyelem != None:\n",
    "        county = ctyelem.text.strip()\n",
    "    else:\n",
    "        county = ''\n",
    "    #date\n",
    "    dateelem = soup.find(\"div\", class_='ddeath')\n",
    "    if dateelem == None:\n",
    "        dateelem = soup.find(\"div\", class_='dpubl')\n",
    "    s = dateelem.text.strip()\n",
    "    try:\n",
    "        date = list(datefinder.find_dates(s))[0]\n",
    "    except:\n",
    "        date = ''\n",
    "    print (n, date, name, address, county)\n",
    "    return name, date, county, address, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467b9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dn_page_new(n):\n",
    "    \"\"\"Get death notice text from page matching the id number\"\"\"\n",
    "    \n",
    "    url = 'https://rip.ie/death-notice/%s' %n\n",
    "    #print (url)\n",
    "    user_agent = 'Chrome/107.0.5304.110'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    req = requests.get(url,headers=headers)   \n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "   \n",
    "    elem = soup.select('[class*=DeathNotice_person-name]')\n",
    "    #print (elem)\n",
    "    if len(elem) == 0:\n",
    "        return '', '', '', '', ''\n",
    "    else:\n",
    "        name = elem[0].get_text(strip=True)\n",
    "    \n",
    "    #date elements\n",
    "    ddateelem = soup.select('[class*=DeathNotice_dates-death-date]')\n",
    "    pubdateelem = soup.select('[class*=DeathNotice_dates-published-date]')\n",
    "    if len(ddateelem) > 0:\n",
    "        date = ddateelem[0].get_text(strip=True)\n",
    "    elif len(pubdateelem) > 0:\n",
    "        print ('using pub date')\n",
    "        date = pubdateelem[0].get_text(strip=True)\n",
    "    else:\n",
    "        print (n, name, 'no date')\n",
    "        return '', '', '', '', ''        \n",
    "    \n",
    "    date = list(datefinder.find_dates(date))[0]\n",
    "\n",
    "    address = soup.select('[class*=DeathNotice_tags-item]')[0].get_text(strip=True)\n",
    "    #print (address) \n",
    "    county = address.split()[-1]\n",
    "\n",
    "    desc = soup.select('[class*=DeathNotice_description]')\n",
    "    text = []\n",
    "    for elem in desc:\n",
    "        text.append(elem.get_text(strip=True))\n",
    "    text = ' '.join(text)\n",
    "    #print (text)\n",
    "   \n",
    "    print (n, date, name, address, county)\n",
    "    return name, date, county, address, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c32f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_dn_page_new(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('rip_dn_scrape.pkl')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adf8c9b4-79b6-4664-8628-dbb56ac7c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123913\n"
     ]
    }
   ],
   "source": [
    "#read current table in so we skip those already done\n",
    "df = pd.read_pickle('rip_dn_scrape_new.pkl')\n",
    "#df = pd.read_parquet('rip_dn_scrape.parquet')\n",
    "print (len(df))\n",
    "ids = list(df.id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "306e0b4a",
   "metadata": {},
   "source": [
    "## iterate over a range of ids to get info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274a23b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ids(start, end):\n",
    "    \"\"\"fetch a range of ids\"\"\"\n",
    "    results={}\n",
    "    for n in range(start, end):\n",
    "        if n in ids:\n",
    "            continue\n",
    "        name,date,cty,addr,txt = get_dn_page_new(n)   \n",
    "        if name == '':\n",
    "            continue\n",
    "        results[n] = [name,date,cty,addr,txt]\n",
    "        time.sleep(0.04)\n",
    "    res = pd.DataFrame.from_dict(results,orient='index',columns=['name','date','county','address','notice']).reset_index()\n",
    "    res = res.rename(columns={'index':'id'})\n",
    "    return res\n",
    "    \n",
    "#res = get_ids(72000,73000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438797ee-6493-41c6-a349-093e44cf1628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_ids_parallel(start, end, n_cores=4):\n",
    "    \"\"\"Get ids in blocks in parallel\"\"\"\n",
    "    \n",
    "    #from multiprocessing import Pool\n",
    "    from multiprocessing.pool import ThreadPool as Pool\n",
    "    pool = Pool(n_cores)\n",
    "    x = np.linspace(start,end,n_cores,dtype=int)\n",
    "    blocks=[]\n",
    "    for i in range(len(x)):\n",
    "        if i < len(x)-1:\n",
    "            blocks.append((x[i],x[i+1]-1))    \n",
    "    print (blocks)\n",
    "    funclist = []\n",
    "    for start,end in blocks:\n",
    "        f = pool.apply_async(get_ids, [start, end])\n",
    "        funclist.append(f)\n",
    "    result=[]\n",
    "    for f in funclist:\n",
    "        df = f.get(timeout=None)\n",
    "        result.append(df)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    result = pd.concat(result).sort_values('id')    \n",
    "    print ('finished')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7eeb21-6214-48bd-81cd-d0e46cd6bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read current table in so we skip those already done\n",
    "df = pd.read_pickle('rip_dn_scrape.pkl')\n",
    "#df = pd.read_parquet('rip_dn_scrape.parquet')\n",
    "print (len(df))\n",
    "ids = list(df.id)\n",
    "#res = get_ids_parallel(530000,537000, n_cores=16)\n",
    "res = get_ids(537000,529500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dad498-5b65-4d7b-af74-46c44c20ea8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc727d40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537095 89 537184\n"
     ]
    }
   ],
   "source": [
    "new = pd.concat([df,res]).reset_index(drop=True)\n",
    "new=new[~new.id.duplicated(keep='first')]\n",
    "#new=new.replace('',None).dropna(axis=0,subset=['date'])\n",
    "#new['date'] = pd.to_datetime(new['date']).apply(lambda x: x.strftime('%d/%m/%Y'))\n",
    "print (len(df),len(res),len(new))\n",
    "#new.to_parquet('rip_dn_scrape.parquet')\n",
    "new.to_pickle('rip_dn_scrape.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7887215a",
   "metadata": {},
   "source": [
    "## clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "820b8741",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537184\n",
      "519427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farrell/.local/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "x=new\n",
    "print (len(x))\n",
    "x=x.replace('',None).dropna(axis=0,subset=['date'])\n",
    "x['date'] = pd.to_datetime(x['date'],format='mixed', errors='coerce')#.apply(lambda x: x.strftime('%d/%m/%Y'))\n",
    "x=x.drop_duplicates(['name','notice'])\n",
    "x=x.drop_duplicates(['name','address'])\n",
    "x=x.drop_duplicates(['name','date','county'])\n",
    "x = x[~x.address.isnull()]\n",
    "nc = ['Fermanagh','Armagh','Tyrone','Down','Antrim','Derry']\n",
    "x = x[~x.county.isin(nc)]\n",
    "x = x[~x.address.str.contains('|'.join(nc))]\n",
    "x=x.sort_values('id')\n",
    "print (len(x))\n",
    "#x.to_pickle('rip_dn_scrape_processed.pkl')\n",
    "x.to_parquet('rip_dn_scrape_processed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b16be-33b6-4796-af28-3fdb68db7767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
