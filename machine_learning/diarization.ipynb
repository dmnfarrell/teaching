{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a05ad53-b463-4913-b7f0-c401bdc43c4f",
   "metadata": {},
   "source": [
    "# openai whisper + pyannote\n",
    "\n",
    "* https://github.com/openai/whisper\n",
    "* https://github.com/pyannote/pyannote-audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f72fa83-b5d3-4a98-89b6-90bc7442dd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/local/stablediff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime, glob, os\n",
    "import subprocess\n",
    "# send pipeline to GPU (when available)\n",
    "import torch\n",
    "import whisper\n",
    "import pyannote.audio\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Segment\n",
    "\n",
    "import wave\n",
    "import contextlib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937de523-1b6e-47c5-ac19-f4a5d8611b11",
   "metadata": {},
   "source": [
    "## whisper speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8594f6cb-d59f-4230-8432-9948ddf4a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_whisper():\n",
    "\tfiles = glob.glob('*.mp4')\n",
    "\tcmd = ''\n",
    "\tfor f in files:\n",
    "\t\tname=os.path.splitext(f)[0]\n",
    "\t\tprint (name)\n",
    "\t\tif os.path.exists(name):\n",
    "\t\t\tcontinue\n",
    "\t\tcmd += 'whisper {} --device cuda --model medium --language en -o {} && '.format(f,name)\n",
    "\t\t#print (cmd)\n",
    "\t\t#subprocess.check_output(cmd,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249c180-d48b-42a6-91e2-f325ff1ea0ab",
   "metadata": {},
   "source": [
    "## basic diarization with pyannote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a00e85b-766b-49ed-ada6-8125eae1a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.0\",num_speakers=2,\n",
    "    use_auth_token=\"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\")\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "# apply pretrained pipeline\n",
    "diarization = pipeline(\"audio.wav\")\n",
    "\n",
    "# print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73ec95ad-1601-4763-9950-5bb39b49edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"audio.rttm\", \"w\") as rttm:\n",
    "    diarization.write_rttm(rttm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8e609-5eb1-481b-93f2-5a9440013edc",
   "metadata": {},
   "source": [
    "## extract speakers with whisper/pyannote\n",
    "\n",
    "https://colab.research.google.com/drive/1V-Bt5Hm2kjaDb4P1RyMSswsDKyrzc2-3?usp=sharing#scrollTo=buGt4moR5Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ddba543-006e-465f-b26e-7928d0fdbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "embedding_model = PretrainedSpeakerEmbedding( \n",
    "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd07e9b9-ec94-4d9d-8c33-876414e13ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'English' #@param ['any', 'English']\n",
    "model_size = 'medium' #@param ['tiny', 'base', 'small', 'medium', 'large']\n",
    "model = whisper.load_model('medium.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997b85fa-fe47-4a7d-bd3f-25fee5bbb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speakers(model, path, num_speakers=2):\n",
    "    \"\"\"Do diarization with speaker names\"\"\"\n",
    "    \n",
    "    mono = 'mono.wav'\n",
    "    cmd = 'ffmpeg -i {} -y -ac 1 mono.wav'.format(path)\n",
    "    subprocess.check_output(cmd, shell=True)\n",
    "    result = model.transcribe(mono)\n",
    "    segments = result[\"segments\"]\n",
    "    \n",
    "    with contextlib.closing(wave.open(mono,'r')) as f:\n",
    "      frames = f.getnframes()\n",
    "      rate = f.getframerate()\n",
    "      duration = frames / float(rate)\n",
    "        \n",
    "    audio = Audio()\n",
    "    def segment_embedding(segment):\n",
    "        start = segment[\"start\"]\n",
    "        # Whisper overshoots the end timestamp in the last segment\n",
    "        end = min(duration, segment[\"end\"])\n",
    "        clip = Segment(start, end)\n",
    "        waveform, sample_rate = audio.crop(mono, clip)\n",
    "        return embedding_model(waveform[None])\n",
    "\n",
    "    embeddings = np.zeros(shape=(len(segments), 192))\n",
    "    for i, segment in enumerate(segments):\n",
    "      embeddings[i] = segment_embedding(segment)\n",
    "    embeddings = np.nan_to_num(embeddings)\n",
    "    \n",
    "    clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
    "    labels = clustering.labels_\n",
    "    for i in range(len(segments)):\n",
    "      segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
    "    return segments    \n",
    "\n",
    "def write_segments(segments, outfile):\n",
    "    \"\"\"write out segments to file\"\"\"\n",
    "    \n",
    "    def time(secs):\n",
    "      return datetime.timedelta(seconds=round(secs))\n",
    "    \n",
    "    f = open(outfile, \"w\")    \n",
    "    for (i, segment) in enumerate(segments):\n",
    "      if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
    "        f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n')\n",
    "      f.write(segment[\"text\"][1:] + ' ')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403c9ce-728d-473f-ab9b-2cab0c4712e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = extract_speakers(model, 'vidal.wav')\n",
    "write_segments(seg, 'transcript.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b706a8f-f54f-483b-a914-34b428a798e5",
   "metadata": {},
   "source": [
    "## convert mp4 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd99584-a037-4c4f-bdb4-676517502469",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('*.wav')\n",
    "for f in files:\n",
    "    name=os.path.splitext(f)[0]        \n",
    "    out = '%s.txt' %name\n",
    "    if not os.path.exists(out):\n",
    "        print (name)\n",
    "        seg = extract_speakers(model, f)\n",
    "        write_segments(seg, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842bbabb-af0f-4c61-9637-7c1ab341dfcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stablediff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
